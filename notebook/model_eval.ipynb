{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from base_fns import get_local_folder\n",
    "os.chdir(os.path.dirname(get_local_folder()))\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import tabulate as tb\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import random\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "from script.ct_model import CtDataset, CtDataModule,CtModel, CtTagger\n",
    "from script.eval import evaluate\n",
    "import torch\n",
    "# from transformers import BertTokenizerFast as BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from torchmetrics import F1Score, Accuracy\n",
    "from sklearn.metrics import classification_report, precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "with initialize(\n",
    "    version_base=None,\n",
    "    config_path=\"../cfg\",\n",
    "):\n",
    "    cfg = compose(config_name=\"main\")\n",
    "    \n",
    "np.random.seed(cfg.random.seed)\n",
    "random.seed(cfg.random.seed)\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS=cfg.model.labels\n",
    "MAX_TOKEN_COUNT = cfg.model.max_token_count\n",
    "THRESHOLD = cfg.model.threshold\n",
    "TEST_FP = cfg.data.processed.text.test\n",
    "test_df = pd.read_json(TEST_FP, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<ml_ab/ft/pmbert>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:10<00:00, 58.78it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "stub_dict = {\"train_silver\" : \"silver\",\n",
    "             \"train_bronze\" : \"bronze\",\n",
    "             \"train_ml_ab\": \"ml_ab\",\n",
    "             \"fulltext\": \"ft\",\n",
    "             \"abstract\": \"abs\"\n",
    "             }\n",
    "\n",
    "\n",
    "for dataset, v in cfg.checkpoint.items():\n",
    "    for text_col, v in cfg.checkpoint[dataset].items():\n",
    "        TEXT_COL = cfg.model.text_col[text_col]\n",
    "        for lm, ckpt in cfg.checkpoint[dataset][text_col].items(): \n",
    "\n",
    "            stub = f\"{stub_dict[dataset]}/{stub_dict[text_col]}/{lm}\"\n",
    "            print(f\"<<{stub}>>\")\n",
    "            CHECKPOINT_PATH = ckpt\n",
    "            LM_MODEL_NAME = cfg.model.lm[lm]\n",
    "            tagger = CtTagger(CHECKPOINT_PATH, LABELS, TEXT_COL, LM_MODEL_NAME)\n",
    "            predictions, labels = tagger.predict(test_df)\n",
    "            y_pred_score = predictions.numpy().flatten()\n",
    "            y_true = labels.numpy().flatten()\n",
    "            results[stub] = evaluate(y_true, y_pred_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>P10</th>\n",
       "      <th>P100</th>\n",
       "      <th>P200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ml_ab/ft/pmbert</th>\n",
       "      <td>0.5665</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.7179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 precision  recall      F1  P10  P100  P200\n",
       "ml_ab/ft/pmbert     0.5665    0.98  0.7179  1.0  0.97  0.88"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_FP = cfg.evaluation.model_eval.results_fp\n",
    "df =pd.DataFrame.from_dict(results, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_FP, \"w\") as f:\n",
    "    json_str = df.to_json(orient='records', lines=True)\n",
    "    f.write(json_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
