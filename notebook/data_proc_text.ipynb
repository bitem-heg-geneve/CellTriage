{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from base_fns import get_local_folder\n",
    "os.chdir(os.path.dirname(get_local_folder()))\n",
    "\n",
    "import pandas as pd\n",
    "import tabulate as tb\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import random\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "from orjson import loads\n",
    "from pathlib import Path\n",
    "\n",
    "with initialize(\n",
    "    version_base=None,\n",
    "    config_path=\"../cfg\",\n",
    "):\n",
    "    cfg = compose(config_name=\"main\")\n",
    "\n",
    "random.seed(cfg.random.seed)\n",
    "np.random.seed(cfg.random.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATASET          SOURCE\n",
       "train_bronze_ab  0         10000\n",
       "                 1         10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmids = Munch() \n",
    "for k, v in cfg.data.processed.pmid.items():\n",
    "        pmids[k] = pd.read_json(v, lines=True)\n",
    "        pmids[k]['DATASET'] = k\n",
    "       \n",
    "pmids_combined = pd.concat([pmids[d]for d in pmids], ignore_index=True)\n",
    "\n",
    "pmids_combined[['DATASET', 'SOURCE']].groupby(['DATASET','SOURCE']).value_counts()\n",
    "# pmids.combined.groupby('ORIGIN').value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repository contains text for 132361 articles\n"
     ]
    }
   ],
   "source": [
    "repo_fn = os.listdir(cfg.data.raw.article)\n",
    "print(f\"repository contains text for {len(repo_fn)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract and fulltext datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()\n",
    "\n",
    "def get_article(row):\n",
    "    article_fn = str(row.PMID) + '.json'\n",
    "    repo_fn = os.listdir(cfg.data.raw.article)\n",
    "    if article_fn in repo_fn:\n",
    "        with open(os.path.join(cfg.data.raw.article, article_fn)) as f:\n",
    "            article = loads(f.read())\n",
    "            # if fulltext is not returned then used abstract.\n",
    "            if not article[\"FULLTEXT\"]:\n",
    "                article[\"FULLTEXT\"] = article[\"ABSTRACT\"]\n",
    "            \n",
    "            return pd.Series(article) \n",
    "    else:\n",
    "        return pd.Series({}) \n",
    "\n",
    "def data_text(df_in):\n",
    "    df_article = df_in.parallel_apply(get_article, axis=1, result_type=\"expand\")\n",
    "    df_article = df_article.drop('PMID', axis=1)\n",
    "    return pd.concat([df_in, df_article], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = Munch()\n",
    "for k, v in pmids.items():\n",
    "   processed[k] = data_text(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples without abstract-text.\n",
    "\n",
    "Samples with no abstract (count, portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATASET          SOURCE  ORIGIN        \n",
       "train_bronze_ab  0       googlescholar       (0, 0.0)\n",
       "                         litsuggest        (61, 0.09)\n",
       "                         medline             (0, 0.0)\n",
       "                 1       cellosaurus_ab      (1, 0.0)\n",
       "Name: ABSTRACT, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_combined = pd.concat([v for k,v in processed.items()], axis=0)\n",
    "processed_combined.groupby([\"DATASET\", \"SOURCE\", \"ORIGIN\"])['ABSTRACT'].apply(lambda x: ((x.isnull().sum()), x.isnull().mean().round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop samples with no abstract and verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATASET          SOURCE  ORIGIN        \n",
       "train_bronze_ab  0       googlescholar     (0, 0.0)\n",
       "                         litsuggest        (0, 0.0)\n",
       "                         medline           (0, 0.0)\n",
       "                 1       cellosaurus_ab    (0, 0.0)\n",
       "Name: ABSTRACT, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in processed.items():\n",
    "    processed[k] = v[v.ABSTRACT.notnull()]\n",
    "\n",
    "processed_combined = pd.concat([v for k,v in processed.items()], axis=0)\n",
    "processed_combined.groupby([\"DATASET\", \"SOURCE\", \"ORIGIN\"])['ABSTRACT'].apply(lambda x: ((x.isnull().sum()), x.isnull().mean().round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATASET          SOURCE  ORIGIN        \n",
       "train_bronze_ab  0       googlescholar      (0, 175, 0.0)\n",
       "                         litsuggest         (0, 584, 0.0)\n",
       "                         medline           (0, 9180, 0.0)\n",
       "                 1       cellosaurus_ab    (0, 9999, 0.0)\n",
       "Name: FULLTEXT, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "processed_combined.groupby([\"DATASET\", \"SOURCE\", 'ORIGIN'])['FULLTEXT'].apply(lambda x: ((x.isnull().sum(), len(x), x.isnull().mean().round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATASET\n",
       "train_bronze_ab    (0, 19938, 0.0)\n",
       "Name: FULLTEXT, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "processed_combined.groupby([\"DATASET\"])['FULLTEXT'].apply(lambda x: ((x.isnull().sum(),len(x), x.isnull().mean().round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write processed data to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write data/processed/text/train_bronze_ab.ndjson\n"
     ]
    }
   ],
   "source": [
    "for k in processed:\n",
    "    processed[k].to_json(cfg.data.processed.text[k], lines=True, orient='records')\n",
    "    print (f'Write {cfg.data.processed.text[k]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
